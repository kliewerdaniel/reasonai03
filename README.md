# ğŸ§  Next-Ollama-App

[![Next.js](https://img.shields.io/badge/Next.js-15.2.1-black)](https://nextjs.org/)
[![Ollama](https://img.shields.io/badge/Ollama-Integration-blue)](https://ollama.com/)
[![React](https://img.shields.io/badge/React-19.0.0-61DAFB)](https://react.dev/)
[![TypeScript](https://img.shields.io/badge/TypeScript-5-3178C6)](https://www.typescriptlang.org/)
[![TailwindCSS](https://img.shields.io/badge/TailwindCSS-4-38B2AC)](https://tailwindcss.com/)

A powerful, privacy-focused AI agent framework built with Next.js and Ollama. This application autonomously decomposes complex tasks into manageable steps, executes them with transparent reasoning, and delivers comprehensive resultsâ€”all while running locally on your machine.

![Reasoning AI Application Interface](ui01.png)

## âœ¨ Features

- **ğŸ“‹ Task Decomposition**: Automatically breaks down user goals into logical, sequential steps
- **ğŸ¤” Transparent Reasoning**: Provides clear reasoning for each step of the process
- **âš¡ Real-time Streaming**: Delivers progress updates as they happen
- **ğŸ”’ Privacy-Focused**: Runs 100% locally through Ollamaâ€”no API keys or data sharing
- **ğŸ’¾ Contextual Memory**: Maintains awareness of previous steps for coherent reasoning
- **ğŸš€ Model Selection**: Flexibility to choose from any model available in your Ollama installation

## ğŸ“‹ Table of Contents

- [Installation](#-installation)
- [Usage](#-usage)
- [Technical Architecture](#-technical-architecture)
- [Core Components](#-core-components)
- [API Endpoints](#-api-endpoints)
- [Customization](#-customization)
- [Development](#-development)
- [Contributing](#-contributing)
- [License](#-license)

## ğŸš€ Installation

### Prerequisites

- [Node.js](https://nodejs.org/) (v20 or later)
- [Ollama](https://ollama.com/) installed and running locally

### Setup

1. **Clone the repository**

```bash
git clone https://github.com/yourusername/next-ollama-app.git
cd next-ollama-app
```

2. **Install dependencies**

```bash
npm install
```

3. **Pull the necessary model with Ollama**

```bash
ollama pull mistral
```

> You can pull additional models if desired: `ollama pull llama3` or `ollama pull gemma:7b`

4. **Start the development server**

```bash
npm run dev
```

5. **Access the application**

Open [http://localhost:3000](http://localhost:3000) in your browser

## ğŸ® Usage

### Executing a Task

1. **Define a goal**: Enter a specific task in the input field (Example: "Research and summarize recent advancements in quantum computing")

2. **Select a model**: Choose from available Ollama models in the dropdown

3. **Execute**: Click "Run Agent" to start the process

4. **Monitor progress**: 
   - Watch as the agent breaks down the task
   - See the reasoning for each step
   - View results as they're generated

5. **Review the final output**: A comprehensive, well-formatted result based on your goal

## ğŸ—ï¸ Technical Architecture

The application uses a modern web architecture with these key components:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Next.js Frontend   â”‚â—„â”€â”€â”€â”€â–ºâ”‚  Next.js API    â”‚â—„â”€â”€â”€â”€â–ºâ”‚ Ollama API    â”‚
â”‚  (React Components) â”‚      â”‚  (Agent Logic)  â”‚      â”‚ (Local Models)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- **Frontend**: React components with streaming UI updates
- **Backend**: Next.js API routes implementing agent logic and Ollama communication
- **Processing**: Ollama running locally for LLM inference

## ğŸ§© Core Components

### Agent Class

The Agent class (`src/lib/agent.ts`) is the heart of the application:

```typescript
// Key components of the Agent class
export class Agent {
  // Initializes with user goal, model selection, and callbacks
  constructor(options: {
    goal: string;
    maxSteps?: number;
    onStepComplete?: StepCallback;
    model?: string;
  }) { ... }

  // Main execution function
  async execute(): Promise<AgentResult> {
    // 1. Analyze task and break into steps
    // 2. For each step:
    //    - Generate reasoning
    //    - Execute step
    //    - Provide output
    // 3. Create comprehensive final output
  }

  // Communication with Ollama
  private async callOllama(prompt: string): Promise<string> { ... }
}
```

### API Routes

The application has two main API endpoints:

1. **`/api/run-agent`**: Executes the agent with streaming responses
2. **`/api/models`**: Fetches available models from Ollama

### Streaming Implementation

The application uses the Web Streams API to deliver real-time updates:

```typescript
// Streaming in run-agent API route
const stream = new TransformStream();
const writer = stream.writable.getWriter();

// Function to send updates to the client
const sendUpdate = async (data: any) => {
  await writer.write(encoder.encode(JSON.stringify(data) + '\n'));
};

// Return stream to client
return new NextResponse(stream.readable, {
  headers: {
    'Content-Type': 'application/json',
    'Transfer-Encoding': 'chunked',
  },
});
```

## ğŸ“¡ API Endpoints

### GET `/api/models`

Returns a list of all available models from your local Ollama installation.

**Response:**
```json
{
  "models": [
    {"name": "mistral:latest", "details": {...}},
    {"name": "llama3:latest", "details": {...}}
  ]
}
```

### POST `/api/run-agent`

Executes the agent with the specified goal and model.

**Request:**
```json
{
  "goal": "Research and summarize quantum computing advancements",
  "model": "mistral:latest"
}
```

**Streaming Response:**
```json
{"type": "log", "message": "ğŸ§  Analyzing task: \"Research and...\""}
{"type": "log", "message": "ğŸ“ Step 1: Define quantum computing..."}
{"type": "log", "message": "ğŸ¤” Reasoning: I'll start by..."}
...
{"type": "result", "content": "# Quantum Computing Advancements\n\n..."}
```

## ğŸ”§ Customization

### Modifying the Agent

The Agent class is designed to be extensible:

```typescript
// Example: Adding a custom task decomposition method
export class CustomAgent extends Agent {
  async customTaskAnalysis(goal: string) {
    // Implement your custom logic
    return [...customSteps];
  }
  
  async execute(): Promise<AgentResult> {
    // Override with custom implementation
    const steps = await this.customTaskAnalysis(this.goal);
    // Continue with execution...
  }
}
```

### Adding New Capabilities

You can extend the agent's capabilities by:

1. **Adding new step types**: Modify the Agent class to handle specialized steps
2. **Integrating external APIs**: Add API calls within the execution of specific steps
3. **Customizing prompting**: Refine the prompts used for task analysis and reasoning

## ğŸ’» Development

### Running in Development Mode

```bash
npm run dev --turbopack
```

### Building for Production

```bash
npm run build
npm start
```

### Linting

```bash
npm run lint
```

## ğŸ‘¥ Contributing

Contributions are welcome! To contribute:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

ğŸ“Œ **Note**: This application requires Ollama to be running locally. If you encounter connection issues, ensure Ollama is running with `ollama serve` and accessible at http://localhost:11434.
